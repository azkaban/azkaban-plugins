package azkaban.jobtype;

/*
 * Copyright 2012 LinkedIn Corp.
 * 
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 * 
 * http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

import azkaban.jobExecutor.ProcessJob;
import azkaban.utils.JSONUtils;
import azkaban.utils.Props;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.security.token.Token;
import org.apache.log4j.ConsoleAppender;
import org.apache.log4j.Layout;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.log4j.PatternLayout;

import azkaban.security.commons.HadoopSecurityManager;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.Writer;
import java.util.LinkedHashMap;
import java.util.Map;
import java.util.Properties;

public abstract class AbstractHadoopJavaMain {

	public static final String JOB_CLASS = "job.class";

	// This is the Job interface method to get the properties generated by the
	// job.
	public static final String GET_GENERATED_PROPERTIES_METHOD = "getJobGeneratedProperties";

	public static final String CANCEL_METHOD_PARAM = "method.cancel";
	public static final String RUN_METHOD_PARAM = "method.run";
	public static final String[] PROPS_CLASSES = new String[] { "azkaban.utils.Props", "azkaban.common.utils.Props" };

	protected static final Layout DEFAULT_LAYOUT = new PatternLayout("%p %m\n");

	protected final Logger logger = Logger.getRootLogger();;

	protected String _runMethod;
	protected String _cancelMethod;
	protected String jobName;
	protected boolean isFinished = false;
	
	protected boolean securityEnabled = false;
	protected Properties prop = new Properties();
	
	protected boolean useToken = false;
	protected boolean useKeytab = false;
	protected boolean shouldProxy = false;
	protected String userToProxy;
	
	protected void setup() throws Exception {

		logger.removeAllAppenders();
		ConsoleAppender appender = new ConsoleAppender(DEFAULT_LAYOUT);
		appender.activateOptions();
		logger.addAppender(appender);
		
		jobName = System.getenv(ProcessJob.JOB_NAME_ENV);
		
		String propsFile = System.getenv(ProcessJob.JOB_PROP_ENV);
		prop.load(new BufferedReader(new FileReader(propsFile)));
		useToken = Boolean.valueOf(prop.getProperty("use.token", "true"));
		useKeytab = Boolean.valueOf(prop.getProperty("use.keytab", "false"));
		shouldProxy = Boolean.valueOf(prop.getProperty(HadoopSecurityManager.SHOULD_PROXY, "true"));
		userToProxy = prop.getProperty("user.to.proxy");
		
		final Configuration conf = new Configuration();
		UserGroupInformation.setConfiguration(conf);
		securityEnabled = UserGroupInformation.isSecurityEnabled();
	}
	
	protected UserGroupInformation getUserUGI() throws Exception {
		UserGroupInformation userUGI = null;
		if(shouldProxy == false) {
			return UserGroupInformation.getCurrentUser();
		}
		logger.info("Proxying enabled.");
		if(securityEnabled == true) {
			logger.info("Hadoop Security enabled.");
			if(useToken == true) {
				String filelocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);
				if(filelocation == null) {
					throw new Exception("Hadoop token file location must be set!");
				}
				logger.info("Found token file " + filelocation);
				logger.info("Setting mapreduce.job.credentials.binary to " + filelocation);
				System.setProperty("mapreduce.job.credentials.binary", filelocation);

				UserGroupInformation loginUser = UserGroupInformation.getLoginUser();
				logger.info("Current logged in user is " + loginUser.getUserName());
				userUGI = UserGroupInformation.createProxyUser(userToProxy, loginUser);
				for (Token<?> token: loginUser.getTokens()) {
					userUGI.addToken(token);
				}
			} else if(useKeytab == true){
				String keytab = prop.getProperty(HadoopSecurityManager.AZKABAN_KEYTAB_LOCATION);
				String principal = prop.getProperty(HadoopSecurityManager.AZKABAN_PRINCIPAL);
				if(keytab == null || principal == null) {
					throw new Exception("Kerberos login info missing: keytab " + keytab + " principal " + principal);
				}
				logger.setLevel(Level.ERROR);
				UserGroupInformation.loginUserFromKeytab(principal, keytab);
				logger.setLevel(Level.INFO);
				userUGI = UserGroupInformation.createProxyUser(userToProxy, UserGroupInformation.getLoginUser());
			} else {
				throw new Exception("No credentials provided for secure hadoop.");
			}
		} else {
			UserGroupInformation.createRemoteUser(userToProxy);
		}
		logger.info("Proxied as user " + userToProxy);
		return userUGI;
	}

	protected void outputGeneratedProperties(Props outputProperties) {
		logger.info("Outputting generated properties to " + ProcessJob.JOB_OUTPUT_PROP_FILE);

		if (outputProperties == null) {
			logger.info("  no gend props");
			return;
		}
		for (String key : outputProperties.getKeySet()) {
			logger.info("  gend prop " + key + " value:" + outputProperties.get(key));
		}

		String outputFileStr = System.getenv(ProcessJob.JOB_OUTPUT_PROP_FILE);
		if (outputFileStr == null) {
			return;
		}

		Map<String, String> properties = new LinkedHashMap<String, String>();
		for (String key : outputProperties.getKeySet()) {
			properties.put(key, outputProperties.get(key));
		}

		Writer writer = null;
		try {
			writer = new BufferedWriter(new FileWriter(outputFileStr));
			JSONUtils.writePropsNoJarDependency(properties, writer);
		} catch (Exception e) {
			new RuntimeException("Unable to store output properties to: " + outputFileStr);
		} finally {
			if (writer != null) {
				try {
					writer.close();
				} catch (IOException e) {
				}
			}
		}
	}

	protected static Class<?> getClass(String className) throws Exception {
		Class<?> runningClass = HadoopJavaJobRunnerMain.class.getClassLoader().loadClass(className);
		if (runningClass == null) {
			throw new Exception("Class " + className + " was not found. Cannot run job.");
		}
		return runningClass;
	}
	
}

